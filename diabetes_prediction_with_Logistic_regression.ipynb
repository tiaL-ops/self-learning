{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic regression:\n",
        "In this notebook I referenced from [Machine learning from scratch](https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/tree/main/03%20Logistic%20Regression), and added a personal study to obtain a better accuracy with a a better learning rate.\n",
        "I used the algorythm on a clean and small dataset as i wanted to focus more on learning how logistic regression works. Data was downloaded from [kaggle ](https://https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset).\n",
        "\n",
        "**About the dataset:**\n",
        "\n",
        "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney\n",
        "Diseases. The objective of the dataset is to diagnostically predict whether a patient has diabetes,\n",
        "based on certain diagnostic measurements included in the dataset. Several constraints were placed\n",
        "on the selection of these instances from a larger database. In particular, all patients here are females\n",
        "at least 21 years old of Pima Indian heritage.2\n",
        "From the data set in the (.csv) File We can find several variables, some of them are independent\n",
        "(several medical predictor variables) and only one target dependent variable (Outcome)\n",
        "\n",
        "**What I am trying to achieve:**\n",
        "From this dataset , I will develop an algorythm that can predict if yes or no a patients has diabetes based on data provided"
      ],
      "metadata": {
        "id": "rh2GE3sXRBOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "# visualisation\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_style('darkgrid')\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "M76dyXxNMw4y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Mk_I7wTkMdOF"
      },
      "outputs": [],
      "source": [
        "data_path=\"/content/drive/MyDrive/Personal Project/diabetes project/diabetes.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(data_path)\n",
        "predict= df.loc[:,df.columns !='Outcome']\n",
        "target=df['Outcome']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(predict, target, test_size=0.15, random_state=42)"
      ],
      "metadata": {
        "id": "Ji3MT8U1MoGf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "peEjTPCcT6bB",
        "outputId": "70886c26-1630-41ee-d9b0-12b1923b0161"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-090a321f-cb60-456a-b379-3b9f75943e87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-090a321f-cb60-456a-b379-3b9f75943e87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-090a321f-cb60-456a-b379-3b9f75943e87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-090a321f-cb60-456a-b379-3b9f75943e87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "class LogisticRegression():\n",
        "\n",
        "    def __init__(self, lr=0.001, n_iters=1000):\n",
        "        self.lr = lr\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.n_iters):\n",
        "            linear_pred = np.dot(X, self.weights) + self.bias\n",
        "            predictions = sigmoid(linear_pred)\n",
        "\n",
        "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
        "            db = (1/n_samples) * np.sum(predictions-y)\n",
        "\n",
        "            self.weights = self.weights - self.lr*dw\n",
        "            self.bias = self.bias - self.lr*db\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_pred = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = sigmoid(linear_pred)\n",
        "        class_pred = [0 if y<=0.5 else 1 for y in y_pred]\n",
        "        return class_pred"
      ],
      "metadata": {
        "id": "OxgDtRmkMftH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "    return np.sum(y_pred==y_test)/len(y_test)"
      ],
      "metadata": {
        "id": "nadzsbUDHMrg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_result={}\n",
        "lr_p=np.arange(0.01, 1, 0.01)\n",
        "for i in lr_p:\n",
        "  clf = LogisticRegression(lr=i)\n",
        "  clf.fit(x_train,y_train)\n",
        "  y_pred = clf.predict(x_test)\n",
        "  acc = accuracy(y_pred, y_test)\n",
        "  accuracy_result[i] = acc\n",
        "  learning_rate=max(accuracy_result, key=accuracy_result.get)\n",
        "\n",
        "print(accuracy_result)\n",
        "print(f\"The highest accuracy is with the learning rate {learning_rate}, with accuracy of :{accuracy_result[learning_rate]}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uA8HFrifNto8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469f1beb-960f-40f6-bdd4-cffc74d40714"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0.01: 0.3793103448275862, 0.02: 0.6896551724137931, 0.03: 0.6896551724137931, 0.04: 0.35344827586206895, 0.05: 0.3706896551724138, 0.060000000000000005: 0.3706896551724138, 0.06999999999999999: 0.6551724137931034, 0.08: 0.6810344827586207, 0.09: 0.6551724137931034, 0.09999999999999999: 0.6379310344827587, 0.11: 0.35344827586206895, 0.12: 0.6551724137931034, 0.13: 0.6551724137931034, 0.14: 0.6724137931034483, 0.15000000000000002: 0.6896551724137931, 0.16: 0.6637931034482759, 0.17: 0.6551724137931034, 0.18000000000000002: 0.5344827586206896, 0.19: 0.6551724137931034, 0.2: 0.6551724137931034, 0.21000000000000002: 0.4396551724137931, 0.22: 0.6551724137931034, 0.23: 0.6551724137931034, 0.24000000000000002: 0.4396551724137931, 0.25: 0.6551724137931034, 0.26: 0.4051724137931034, 0.27: 0.6551724137931034, 0.28: 0.6551724137931034, 0.29000000000000004: 0.3793103448275862, 0.3: 0.6551724137931034, 0.31: 0.6551724137931034, 0.32: 0.6379310344827587, 0.33: 0.3793103448275862, 0.34: 0.7068965517241379, 0.35000000000000003: 0.6551724137931034, 0.36000000000000004: 0.6551724137931034, 0.37: 0.6551724137931034, 0.38: 0.6551724137931034, 0.39: 0.6551724137931034, 0.4: 0.6551724137931034, 0.41000000000000003: 0.6982758620689655, 0.42000000000000004: 0.47413793103448276, 0.43: 0.4224137931034483, 0.44: 0.6810344827586207, 0.45: 0.6551724137931034, 0.46: 0.6551724137931034, 0.47000000000000003: 0.6637931034482759, 0.48000000000000004: 0.6551724137931034, 0.49: 0.646551724137931, 0.5: 0.6551724137931034, 0.51: 0.6551724137931034, 0.52: 0.6551724137931034, 0.53: 0.5431034482758621, 0.54: 0.6551724137931034, 0.55: 0.6551724137931034, 0.56: 0.6551724137931034, 0.5700000000000001: 0.35344827586206895, 0.5800000000000001: 0.6551724137931034, 0.59: 0.6551724137931034, 0.6: 0.6379310344827587, 0.61: 0.6120689655172413, 0.62: 0.3793103448275862, 0.63: 0.6551724137931034, 0.64: 0.6896551724137931, 0.65: 0.6551724137931034, 0.66: 0.6551724137931034, 0.67: 0.5603448275862069, 0.68: 0.6379310344827587, 0.6900000000000001: 0.6551724137931034, 0.7000000000000001: 0.6551724137931034, 0.7100000000000001: 0.43103448275862066, 0.72: 0.6551724137931034, 0.73: 0.6551724137931034, 0.74: 0.3706896551724138, 0.75: 0.6637931034482759, 0.76: 0.6551724137931034, 0.77: 0.43103448275862066, 0.78: 0.6551724137931034, 0.79: 0.6551724137931034, 0.8: 0.6379310344827587, 0.81: 0.5862068965517241, 0.8200000000000001: 0.5258620689655172, 0.8300000000000001: 0.6810344827586207, 0.8400000000000001: 0.41379310344827586, 0.85: 0.3448275862068966, 0.86: 0.6724137931034483, 0.87: 0.6551724137931034, 0.88: 0.6551724137931034, 0.89: 0.646551724137931, 0.9: 0.6551724137931034, 0.91: 0.6637931034482759, 0.92: 0.3448275862068966, 0.93: 0.4224137931034483, 0.9400000000000001: 0.6551724137931034, 0.9500000000000001: 0.6724137931034483, 0.9600000000000001: 0.6551724137931034, 0.97: 0.6551724137931034, 0.98: 0.6637931034482759, 0.99: 0.3448275862068966}\n",
            "The highest accuracy is with the learning rate 0.34, with accuracy of :0.7068965517241379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(0.01, 1, 0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rkQjz7DSVpF",
        "outputId": "6719298d-8078-4d84-e99a-5fa860f097a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 , 0.11,\n",
              "       0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21, 0.22,\n",
              "       0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32, 0.33,\n",
              "       0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43, 0.44,\n",
              "       0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54, 0.55,\n",
              "       0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65, 0.66,\n",
              "       0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77,\n",
              "       0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88,\n",
              "       0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UMyR7-ZcSZjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}